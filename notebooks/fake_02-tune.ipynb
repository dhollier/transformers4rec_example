{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb18ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ff149",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_transformers4rec_end-to-end-session-based-02-end-to-end-session-based-with-yoochoose-pyt/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# End-to-end session-based recommendations with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe975de6",
   "metadata": {},
   "source": [
    "In recent years, several deep learning-based algorithms have been proposed for recommendation systems while its adoption in industry deployments have been steeply growing. In particular, NLP inspired approaches have been successfully adapted for sequential and session-based recommendation problems, which are important for many domains like e-commerce, news and streaming media. Session-Based Recommender Systems (SBRS) have been proposed to model the sequence of interactions within the current user session, where a session is a short sequence of user interactions typically bounded by user inactivity. They have recently gained popularity due to their ability to capture short-term or contextual user preferences towards items. \n",
    "\n",
    "The field of NLP has evolved significantly within the last decade, particularly due to the increased usage of deep learning. As a result, state of the art NLP approaches have inspired RecSys practitioners and researchers to adapt those architectures, especially for sequential and session-based recommendation problems. Here, we leverage one of the state-of-the-art Transformer-based architecture, [XLNet](https://arxiv.org/abs/1906.08237) with Masked Language Modeling (MLM) training technique (see our [tutorial](https://github.com/NVIDIA-Merlin/Transformers4Rec/tree/main/examples/tutorial) for details) for training a session-based model.\n",
    "\n",
    "In this end-to-end-session-based recommnender model example, we use `Transformers4Rec` library, which leverages the popular [HuggingFaceâ€™s Transformers](https://github.com/huggingface/transformers) NLP library and make it possible to experiment with cutting-edge implementation of such architectures for sequential and session-based recommendation problems. For detailed explanations of the building blocks of Transformers4Rec meta-architecture visit [getting-started-session-based](https://github.com/NVIDIA-Merlin/Transformers4Rec/tree/main/examples/getting-started-session-based) and [tutorial](https://github.com/NVIDIA-Merlin/Transformers4Rec/tree/main/examples/tutorial) example notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7516d46",
   "metadata": {},
   "source": [
    "## 1. Model definition using Transformers4Rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8642b00",
   "metadata": {},
   "source": [
    "In the previous notebook, we have created sequential features and saved our processed data frames as parquet files. Now we use these processed parquet files to train a session-based recommendation model with the XLNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc0fdb",
   "metadata": {},
   "source": [
    "### 1.1 Get the schema "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d52653",
   "metadata": {},
   "source": [
    "The library uses a schema format to configure the input features and automatically creates the necessary layers. This *protobuf* text file contains the description of each input feature by defining: the name, the type, the number of elements of a list column,  the cardinality of a categorical feature and the min and max values of each feature. In addition, the annotation field contains the tags such as specifying the `continuous` and `categorical` features, the `target` column or the `item_id` feature, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51804588-b01f-4b94-8d0b-76deae31540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch \n",
    "from transformers4rec import torch as tr\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt, RecallAt\n",
    "from transformers4rec.torch.utils.examples_utils import wipe_memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd285d57-93ae-4f27-bfaf-9742078dd912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"session_id\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"session_id\"\n",
      "    max: 3714319\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"categorical\"\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"pc9-count\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"pc9\"\n",
      "    max: 14644\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"categorical\"\n",
      "    tag: \"list\"\n",
      "    tag: \"item\"\n",
      "\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"et_dayofweek_sin-first\"\n",
      "  type: FLOAT\n",
      "  annotation {\n",
      "    tag: \"categorical\"\n",
      "    tag: \"list\"\n",
      "    tag: \"item\"\n",
      "\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"pc9-list_seq\"\n",
      "  value_count {\n",
      "  }\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"pc9\"\n",
      "    max: 14644\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"categorical\"\n",
      "    tag: \"id\"\n",
      "    tag: \"item\"\n",
      "    tag: \"list\"\n",
      "    tag: \"item_id\"\n",
      "\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"product_item_type-list_seq\"\n",
      "  value_count {\n",
      "  }\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"product_item_type\"\n",
      "    max: 50\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"categorical\"\n",
      "    tag: \"list\"\n",
      "    tag: \"item\"\n",
      "\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"color_group-list_seq\"\n",
      "  value_count {\n",
      "  }\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"color_group\"\n",
      "    max: 31\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"categorical\"\n",
      "    tag: \"list\"\n",
      "    tag: \"item\"\n",
      "\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"action-list_seq\"\n",
      "  value_count {\n",
      "  }\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"action\"\n",
      "    max: 4\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"categorical\"\n",
      "    tag: \"list\"\n",
      "    tag: \"item\"\n",
      "\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"price_log_norm-list_seq\"\n",
      "  value_count {\n",
      "  }\n",
      "  type: FLOAT\n",
      "  annotation {\n",
      "    tag: \"list\"\n",
      "    tag: \"continuous\"\n",
      "\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"relative_price_to_avg_product_item_type-list_seq\"\n",
      "  value_count {\n",
      "  }\n",
      "  type: FLOAT\n",
      "  annotation {\n",
      "    tag: \"list\"\n",
      "    tag: \"continuous\"\n",
      "\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#if this errors manually edit and remove \"extra meta data\" from the schema file\n",
    "\n",
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = \"../data/schema_generated/schema.pbtxt\"\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "!cat $SCHEMA_PATH\n",
    "\n",
    "# We can select the subset of features we want to use for training the model by their tags or their names.\n",
    "\n",
    "# schema = schema.select_by_name(\n",
    "#    ['pc9-list_seq', \n",
    "#     'product_item_type-list_seq', \n",
    "#     'color_group-list_seq',\n",
    "#     'price_log_norm-list_seq',\n",
    "#     'product_recency_days_log_norm-list_seq',\n",
    "#     'et_dayofweek_sin-list_seq']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ca4c0",
   "metadata": {},
   "source": [
    "### 1.2 Define the end-to-end Session-based Transformer-based recommendation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74ac27",
   "metadata": {
    "tags": []
   },
   "source": [
    "For defining a session-based recommendation model, the end-to-end model definition requires four steps:\n",
    "\n",
    "1. Instantiate [TabularSequenceFeatures](https://nvidia-merlin.github.io/Transformers4Rec/main/api/transformers4rec.tf.features.html?highlight=tabularsequence#transformers4rec.tf.features.sequence.TabularSequenceFeatures) input-module from schema to prepare the embedding tables of categorical variables and project continuous features, if specified. In addition, the module provides different aggregation methods (e.g. 'concat', 'elementwise-sum') to merge input features and generate the sequence of interactions embeddings. The module also supports language modeling tasks to prepare masked labels for training and evaluation (e.g: 'mlm' for masked language modeling) \n",
    "\n",
    "2. Next, we need to define one or multiple prediction tasks. For this demo, we are going to use [NextItemPredictionTask](https://nvidia-merlin.github.io/Transformers4Rec/main/api/transformers4rec.tf.model.html?highlight=nextitem#transformers4rec.tf.model.prediction_task.NextItemPredictionTask) with `Masked Language modeling`: during training, randomly selected items are masked and predicted using the unmasked sequence items. For inference, it is meant to always predict the next item to be interacted with.\n",
    "\n",
    "3. Then we construct a `transformer_config` based on the architectures provided by [Hugging Face Transformers](https://github.com/huggingface/transformers) framework. </a>\n",
    "\n",
    "4. Finally we link the transformer-body to the inputs and the prediction tasks to get the final pytorch `Model` class.\n",
    "    \n",
    "For more details about the features supported by each sub-module, please check out the library [documentation](https://nvidia-merlin.github.io/Transformers4Rec/main/index.html) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79c0e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "max_sequence_length= 20 \n",
    "d_model = 64\n",
    "d_output=100\n",
    "\n",
    "# Define input module to process tabular input-features and to prepare masked inputs\n",
    "inputs = tr.TabularSequenceFeatures.from_schema(\n",
    "    schema,\n",
    "    max_sequence_length=max_sequence_length,\n",
    "    continuous_projection=64,\n",
    "    aggregation=\"concat\",\n",
    "    d_output=d_output,\n",
    "    masking=\"mlm\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a753e582-6d2a-4337-9b8c-8b465acfa66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt, RecallAt\n",
    "\n",
    "# Define XLNetConfig class and set default parameters for HF XLNet config  \n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model=d_model, n_head=4, n_layer=2, total_seq_length=max_sequence_length\n",
    ")\n",
    "# Define the model block including: inputs, masking, projection and transformer block.\n",
    "body = tr.SequentialBlock(\n",
    "    inputs, tr.MLPBlock([64]), tr.TransformerBlock(transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "# Define the evaluation top-N metrics and the cut-offs\n",
    "metrics = [NDCGAt(top_ks=[10, 20], labels_onehot=True),  \n",
    "           RecallAt(top_ks=[10, 20], labels_onehot=True)]\n",
    "\n",
    "# Define a head related to next item prediction task \n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True, \n",
    "                              metrics=metrics),\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "# Get the end-to-end Model class \n",
    "model = tr.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847757ca-3c91-4504-b9dc-29127563da01",
   "metadata": {},
   "source": [
    "You can print out the model structure by uncommenting the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d4a3f7-f495-4504-bfe1-94f1a6cc8328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (heads): ModuleList(\n",
       "    (0): Head(\n",
       "      (body): SequentialBlock(\n",
       "        (0): TabularSequenceFeatures(\n",
       "          (_aggregation): ConcatFeatures()\n",
       "          (to_merge): ModuleDict(\n",
       "            (continuous_module): SequentialBlock(\n",
       "              (0): ContinuousFeatures(\n",
       "                (filter_features): FilterFeatures()\n",
       "                (_aggregation): ConcatFeatures()\n",
       "              )\n",
       "              (1): SequentialBlock(\n",
       "                (0): DenseBlock(\n",
       "                  (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): AsTabular()\n",
       "            )\n",
       "            (categorical_module): SequenceEmbeddingFeatures(\n",
       "              (filter_features): FilterFeatures()\n",
       "              (embedding_tables): ModuleDict(\n",
       "                (session_id): Embedding(3714320, 64, padding_idx=0)\n",
       "                (pc9-count): Embedding(14645, 64, padding_idx=0)\n",
       "                (pc9-list_seq): Embedding(14645, 64, padding_idx=0)\n",
       "                (product_item_type-list_seq): Embedding(51, 64, padding_idx=0)\n",
       "                (color_group-list_seq): Embedding(32, 64, padding_idx=0)\n",
       "                (action-list_seq): Embedding(5, 64, padding_idx=0)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (projection_module): SequentialBlock(\n",
       "            (0): DenseBlock(\n",
       "              (0): Linear(in_features=448, out_features=100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (_masking): MaskedLanguageModeling()\n",
       "        )\n",
       "        (1): SequentialBlock(\n",
       "          (0): DenseBlock(\n",
       "            (0): Linear(in_features=100, out_features=64, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): TansformerBlock(\n",
       "          (transformer): XLNetModel(\n",
       "            (word_embedding): Embedding(1, 64)\n",
       "            (layer): ModuleList(\n",
       "              (0): XLNetLayer(\n",
       "                (rel_attn): XLNetRelativeAttention(\n",
       "                  (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (ff): XLNetFeedForward(\n",
       "                  (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                  (layer_1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (layer_2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "              (1): XLNetLayer(\n",
       "                (rel_attn): XLNetRelativeAttention(\n",
       "                  (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (ff): XLNetFeedForward(\n",
       "                  (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
       "                  (layer_1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (layer_2): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.3, inplace=False)\n",
       "                )\n",
       "                (dropout): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (masking): MaskedLanguageModeling()\n",
       "        )\n",
       "      )\n",
       "      (prediction_task_dict): ModuleDict(\n",
       "        (next-item): NextItemPredictionTask(\n",
       "          (sequence_summary): SequenceSummary(\n",
       "            (summary): Identity()\n",
       "            (activation): Identity()\n",
       "            (first_dropout): Identity()\n",
       "            (last_dropout): Identity()\n",
       "          )\n",
       "          (metrics): ModuleList(\n",
       "            (0): NDCGAt()\n",
       "            (1): RecallAt()\n",
       "          )\n",
       "          (loss): NLLLoss()\n",
       "          (embeddings): SequenceEmbeddingFeatures(\n",
       "            (filter_features): FilterFeatures()\n",
       "            (embedding_tables): ModuleDict(\n",
       "              (session_id): Embedding(3714320, 64, padding_idx=0)\n",
       "              (pc9-count): Embedding(14645, 64, padding_idx=0)\n",
       "              (pc9-list_seq): Embedding(14645, 64, padding_idx=0)\n",
       "              (product_item_type-list_seq): Embedding(51, 64, padding_idx=0)\n",
       "              (color_group-list_seq): Embedding(32, 64, padding_idx=0)\n",
       "              (action-list_seq): Embedding(5, 64, padding_idx=0)\n",
       "            )\n",
       "          )\n",
       "          (item_embedding_table): Embedding(14645, 64, padding_idx=0)\n",
       "          (masking): MaskedLanguageModeling()\n",
       "          (pre): Block(\n",
       "            (module): NextItemPredictionTask(\n",
       "              (item_embedding_table): Embedding(14645, 64, padding_idx=0)\n",
       "              (log_softmax): LogSoftmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa0aa0",
   "metadata": {},
   "source": [
    "### 1.3. Daily Fine-Tuning: Training over a time windowÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecb72a",
   "metadata": {},
   "source": [
    "Now that the model is defined, we are going to launch training. For that, Transfromers4rec extends HF Transformers Trainer class to adapt the evaluation loop for session-based recommendation task and the calculation of ranking metrics. The original `train()` method is not modified meaning that we leverage the efficient training implementation from that library, which manages, for example, half-precision (FP16) training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6186d69",
   "metadata": {},
   "source": [
    "#### Set the training arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818197e2",
   "metadata": {},
   "source": [
    "An additional argument `data_loader_engine` is defined to automatically load the features needed for training using the schema. The default value is `nvtabular` for optimized GPU-based data-loading.  Optionally a `PyarrowDataLoader` (`pyarrow`) can also be used as a basic option, but it is slower and works only for small datasets, as the full data is loaded to CPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c57b0b-f764-4c38-98c9-ab42346e8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set hyperparameters for training\n",
    "training_args = tr.trainer.T4RecTrainingArguments(\n",
    "                                    data_loader_engine='nvtabular',\n",
    "                                    dataloader_drop_last = True,\n",
    "                                    gradient_accumulation_steps = 1,\n",
    "                                    per_device_train_batch_size = 256, \n",
    "                                    per_device_eval_batch_size = 32,\n",
    "                                    output_dir = \"./tmp\", \n",
    "                                    learning_rate=0.0001,\n",
    "                                    lr_scheduler_type='cosine', \n",
    "                                    learning_rate_num_cosine_cycles_by_epoch=1.5,\n",
    "                                    num_train_epochs= 9,\n",
    "                                    max_sequence_length=20, \n",
    "                                    report_to = [],\n",
    "                                    logging_steps=500,\n",
    "                                    no_cuda=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836df805",
   "metadata": {},
   "source": [
    "#### Instantiate the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e707cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = tr.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116800b",
   "metadata": {},
   "source": [
    "#### Launch daily training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709592a2",
   "metadata": {},
   "source": [
    "In this demo, we will use the `fit_and_evaluate` method that allows us to conduct a time-based finetuning by iteratively training and evaluating using a sliding time window: At each iteration, we use the training data of a specific time index $t$ to train the model; then we evaluate on the validation data of the next index $t + 1$. Particularly, we set start time to 178 and end time to 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cd5b5f2-6ffb-433e-862d-d7393618d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3efc7a39-4798-480f-a58e-1cc0fa45ca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>pc9-count</th>\n",
       "      <th>et_dayofweek_sin-first</th>\n",
       "      <th>pc9-list_seq</th>\n",
       "      <th>product_item_type-list_seq</th>\n",
       "      <th>color_group-list_seq</th>\n",
       "      <th>action-list_seq</th>\n",
       "      <th>price_log_norm-list_seq</th>\n",
       "      <th>relative_price_to_avg_product_item_type-list_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>198</td>\n",
       "      <td>299</td>\n",
       "      <td>-0.433885</td>\n",
       "      <td>[1037, 1037, 1037, 1048, 1048, 2286, 2286, 203...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 29, 29, 29, 6, 6, 6, 6, 7, 7, ...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 5, 5, 5, 8, 8, 5, 6, 8, 8, 8, ...</td>\n",
       "      <td>[3, 2, 4, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, ...</td>\n",
       "      <td>[-1.9052631, -1.9052631, -1.9052631, -1.333002...</td>\n",
       "      <td>[-0.44284406, -0.44284406, -0.44284406, -0.263...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>785</td>\n",
       "      <td>187</td>\n",
       "      <td>-0.433885</td>\n",
       "      <td>[8, 8, 7, 1114, 1114, 1114, 381, 381, 381, 788...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[4, 4, 2, 14, 14, 14, 8, 8, 8, 11, 11, 11, 9, ...</td>\n",
       "      <td>[3, 2, 2, 3, 2, 4, 3, 2, 4, 3, 2, 4, 3, 2, 3, ...</td>\n",
       "      <td>[0.2555583, 0.2555583, 0.2555583, -1.833608, -...</td>\n",
       "      <td>[-0.033521444, -0.033521444, -0.033521444, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>847</td>\n",
       "      <td>182</td>\n",
       "      <td>-0.433885</td>\n",
       "      <td>[21, 45, 18, 21, 13, 42, 55, 28, 10, 111, 89, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, ...</td>\n",
       "      <td>[4, 3, 3, 4, 2, 2, 3, 5, 2, 3, 2, 4, 2, 3, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[0.69261944, 0.69261944, 0.89606464, 0.6926194...</td>\n",
       "      <td>[0.1913089, 0.1913089, 0.3129329, 0.1913089, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1031</td>\n",
       "      <td>170</td>\n",
       "      <td>-0.433885</td>\n",
       "      <td>[207, 207, 1581, 1581, 271, 1285, 18, 21, 429,...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 3, 3, 4, 2, 2, 2, 2, 2, 3, 2, ...</td>\n",
       "      <td>[3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, ...</td>\n",
       "      <td>[-0.024955014, -0.024955014, -0.4382261, -0.43...</td>\n",
       "      <td>[-0.15514545, -0.15514545, -0.30731583, -0.307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1579</td>\n",
       "      <td>146</td>\n",
       "      <td>-0.433885</td>\n",
       "      <td>[207, 207, 409, 423, 487, 97, 207, 1297, 507, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[2, 2, 2, 5, 3, 3, 2, 2, 3, 6, 2, 2, 4, 4, 3, ...</td>\n",
       "      <td>[3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, ...</td>\n",
       "      <td>[-0.024955014, -0.024955014, -0.024955014, -0....</td>\n",
       "      <td>[-0.15514545, -0.15514545, -0.15514545, -0.155...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session_id  pc9-count  et_dayofweek_sin-first  \\\n",
       "7          198        299               -0.433885   \n",
       "35         785        187               -0.433885   \n",
       "41         847        182               -0.433885   \n",
       "48        1031        170               -0.433885   \n",
       "67        1579        146               -0.433885   \n",
       "\n",
       "                                         pc9-list_seq  \\\n",
       "7   [1037, 1037, 1037, 1048, 1048, 2286, 2286, 203...   \n",
       "35  [8, 8, 7, 1114, 1114, 1114, 381, 381, 381, 788...   \n",
       "41  [21, 45, 18, 21, 13, 42, 55, 28, 10, 111, 89, ...   \n",
       "48  [207, 207, 1581, 1581, 271, 1285, 18, 21, 429,...   \n",
       "67  [207, 207, 409, 423, 487, 97, 207, 1297, 507, ...   \n",
       "\n",
       "                           product_item_type-list_seq  \\\n",
       "7   [7, 7, 7, 7, 7, 29, 29, 29, 6, 6, 6, 6, 7, 7, ...   \n",
       "35  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "41  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, ...   \n",
       "48  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "67  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                 color_group-list_seq  \\\n",
       "7   [8, 8, 8, 8, 8, 5, 5, 5, 8, 8, 5, 6, 8, 8, 8, ...   \n",
       "35  [4, 4, 2, 14, 14, 14, 8, 8, 8, 11, 11, 11, 9, ...   \n",
       "41  [4, 3, 3, 4, 2, 2, 3, 5, 2, 3, 2, 4, 2, 3, 2, ...   \n",
       "48  [2, 2, 2, 2, 2, 3, 3, 4, 2, 2, 2, 2, 2, 3, 2, ...   \n",
       "67  [2, 2, 2, 5, 3, 3, 2, 2, 3, 6, 2, 2, 4, 4, 3, ...   \n",
       "\n",
       "                                      action-list_seq  \\\n",
       "7   [3, 2, 4, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 3, ...   \n",
       "35  [3, 2, 2, 3, 2, 4, 3, 2, 4, 3, 2, 4, 3, 2, 3, ...   \n",
       "41  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "48  [3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, ...   \n",
       "67  [3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, ...   \n",
       "\n",
       "                              price_log_norm-list_seq  \\\n",
       "7   [-1.9052631, -1.9052631, -1.9052631, -1.333002...   \n",
       "35  [0.2555583, 0.2555583, 0.2555583, -1.833608, -...   \n",
       "41  [0.69261944, 0.69261944, 0.89606464, 0.6926194...   \n",
       "48  [-0.024955014, -0.024955014, -0.4382261, -0.43...   \n",
       "67  [-0.024955014, -0.024955014, -0.024955014, -0....   \n",
       "\n",
       "     relative_price_to_avg_product_item_type-list_seq  \n",
       "7   [-0.44284406, -0.44284406, -0.44284406, -0.263...  \n",
       "35  [-0.033521444, -0.033521444, -0.033521444, -0....  \n",
       "41  [0.1913089, 0.1913089, 0.3129329, 0.1913089, 0...  \n",
       "48  [-0.15514545, -0.15514545, -0.30731583, -0.307...  \n",
       "67  [-0.15514545, -0.15514545, -0.15514545, -0.155...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cudf.io.read_parquet('../data/preproc_sessions_by_day/1/valid.parquet').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a238aae-6619-4004-9f82-4744c6e89eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "Int64Index: 9196 entries, 7 to 91786\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                            Non-Null Count  Dtype\n",
      "---  ------                                            --------------  -----\n",
      " 0   session_id                                        9196 non-null   int64\n",
      " 1   pc9-count                                         9196 non-null   int32\n",
      " 2   et_dayofweek_sin-first                            9196 non-null   float32\n",
      " 3   pc9-list_seq                                      9196 non-null   list\n",
      " 4   product_item_type-list_seq                        9196 non-null   list\n",
      " 5   color_group-list_seq                              9196 non-null   list\n",
      " 6   action-list_seq                                   9196 non-null   list\n",
      " 7   price_log_norm-list_seq                           9196 non-null   list\n",
      " 8   relative_price_to_avg_product_item_type-list_seq  9196 non-null   list\n",
      "dtypes: float32(1), int32(1), int64(1), list(6)\n",
      "memory usage: 7.4 MB\n"
     ]
    }
   ],
   "source": [
    "cudf.io.read_parquet('../data/preproc_sessions_by_day/1/valid.parquet').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00a18f7-a94e-45d5-a5a7-3bf3556fd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "INPUT_DATA_DIR = f'../data'\n",
    "OUTPUT_DIR = f'{INPUT_DATA_DIR}/preproc_sessions_by_day'\n",
    "start_time_window_index = 1\n",
    "final_time_window_index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b23d57d6-6eff-44fe-8544-28efc52a885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/preproc_sessions_by_day/1/train.parquet']\n",
      "********************\n",
      "Launch training for day 1 are:\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 73472\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2583\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2583' max='2583' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2583/2583 07:31, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.847200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>7.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.342300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>7.269600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./tmp/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Saving model checkpoint to ./tmp/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Saving model checkpoint to ./tmp/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Saving model checkpoint to ./tmp/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Saving model checkpoint to ./tmp/checkpoint-2500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='280' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [280/280 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Eval results for day 2 are:\t\n",
      "\n",
      "********************\n",
      "\n",
      " eval_/loss = 7.305938720703125\n",
      " eval_/next-item/ndcg_at_10 = 0.035522446036338806\n",
      " eval_/next-item/ndcg_at_20 = 0.04520158842206001\n",
      " eval_/next-item/recall_at_10 = 0.06450892984867096\n",
      " eval_/next-item/recall_at_20 = 0.10234375298023224\n",
      " eval_runtime = 6.3355\n",
      " eval_samples_per_second = 1414.263\n",
      " eval_steps_per_second = 44.196\n",
      "CPU times: user 2min 12s, sys: 3min 40s, total: 5min 53s\n",
      "Wall time: 7min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Iterating over days of one week\n",
    "for time_index in range(start_time_window_index, final_time_window_index):\n",
    "    # Set data \n",
    "    time_index_train = time_index\n",
    "    time_index_eval = time_index + 1\n",
    "    train_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\"))\n",
    "    eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"))\n",
    "    print(train_paths)\n",
    "    \n",
    "    # Train on day related to time_index \n",
    "    print('*'*20)\n",
    "    print(\"Launch training for day %s are:\" %time_index)\n",
    "    print('*'*20 + '\\n')\n",
    "    trainer.train_dataset_or_path = train_paths\n",
    "    trainer.reset_lr_scheduler()\n",
    "    trainer.train()\n",
    "    trainer.state.global_step +=1\n",
    "    print('finished')\n",
    "    \n",
    "    # Evaluate on the following day\n",
    "    trainer.eval_dataset_or_path = eval_paths\n",
    "    train_metrics = trainer.evaluate(metric_key_prefix='eval')\n",
    "    print('*'*20)\n",
    "    print(\"Eval results for day %s are:\\t\" %time_index_eval)\n",
    "    print('\\n' + '*'*20 + '\\n')\n",
    "    for key in sorted(train_metrics.keys()):\n",
    "        print(\" %s = %s\" % (key, str(train_metrics[key]))) \n",
    "    wipe_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "536cb1f2-bc5a-493c-9d0d-8f6a2acb3c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next-item/ndcg_at_10': tensor(0.0355, device='cuda:0'),\n",
       " 'next-item/ndcg_at_20': tensor(0.0452, device='cuda:0'),\n",
       " 'next-item/recall_at_10': tensor(0.0645, device='cuda:0'),\n",
       " 'next-item/recall_at_20': tensor(0.1023, device='cuda:0')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2909d9a",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c91bf13f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./tmp/checkpoint-2584\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    }
   ],
   "source": [
    "trainer._save_model_and_checkpoint(save_model_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55e66e",
   "metadata": {},
   "source": [
    "#### Export the preprocessing workflow and model in the format required by Triton server:\n",
    "\n",
    "NVTabularâ€™s `export_pytorch_ensemble()` function enables us to create model files and config files to be served to Triton Inference Server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3ee6533-3744-49ec-a682-6f4463480092",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../models\n",
    "!mkdir ../models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5daef25-46e2-4347-bfe9-93df623d0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model metrics\n",
    "with open(\"../models/results.txt\", 'w') as f: \n",
    "    f.write('GRU accuracy results:')\n",
    "    f.write('\\n')\n",
    "    for key, value in  model.compute_metrics().items(): \n",
    "        f.write('%s:%s\\n' % (key, value.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20583e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.inference.triton import export_pytorch_ensemble\n",
    "from nvtabular.workflow import Workflow\n",
    "workflow = Workflow.load(\"../data/workflow_etl\")\n",
    "\n",
    "export_pytorch_ensemble(\n",
    "    model,\n",
    "    workflow,\n",
    "    sparse_max=trainer.get_train_dataloader().dataset.sparse_max,\n",
    "    name= \"t4r_pytorch\",\n",
    "    model_path= \"../models/\",\n",
    "    label_columns =[],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b543a88d374ac88bf8df97911b380f671b13649694a5b49eb21e60fd27eb479"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
